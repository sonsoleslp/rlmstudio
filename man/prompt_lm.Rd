% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hello.R
\name{prompt_lm}
\alias{prompt_lm}
\title{Prompt Language Model Function}
\usage{
prompt_lm(
  prompt,
  endpoint = "http://localhost:1234/v1/completions",
  api_key = "lm-studio",
  max_tokens = 1000,
  verbose = F
)
}
\arguments{
\item{prompt}{A string containing the input text prompt to send to the language model.}

\item{endpoint}{A string specifying the URL of the language model API endpoint. Default is "http://localhost:1234/v1/completions".}

\item{api_key}{A string containing the API key for authorization. Default is "lm-studio".}

\item{max_tokens}{An integer specifying the maximum number of tokens to generate. Default is 1000.}

\item{verbose}{A logical value indicating whether to return the full response content (TRUE) or just the generated text (FALSE). Default is FALSE.}
}
\value{
If \code{verbose} is TRUE, returns a list containing the full response content from the API. If \code{verbose} is FALSE, returns a string of the generated response
}
\description{
This function sends a prompt to a specified language model API endpoint and returns the generated text.
}
\examples{
\dontrun{
# Don't forget to have your LM server running!
# Basic example
prompt_lm("Once upon a time", max_tokens = 50)

# Example with verbose output
prompt_lm("What is the capital of France?", verbose = TRUE)

# Example using the function within a mutate function
library(dplyr)
df <- data.frame(text_prompts = c("Tell me a joke", "Explain the theory of relativity"))
df <- df \%>\% rowwise \%>\%
  mutate(generated_text = prompt_lm(x))
}

}
